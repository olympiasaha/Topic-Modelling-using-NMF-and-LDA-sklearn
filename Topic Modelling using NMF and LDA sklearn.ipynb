{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import functools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"xid-34549157_1\")\n",
    "train['text_processed']=train['text'].map(lambda x:re.sub('[,\\.!?]','',x))\n",
    "train['text_processed']=train['text_processed'].map(lambda x:x.lower())\n",
    "def cleanText(input_string):\n",
    "    modified_string=re.sub('[^A-Za-z0-9]+',' ', input_string)\n",
    "    return(modified_string)\n",
    "train['text_processed']=train.text_processed.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Rup/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords=stopwords.words('english')\n",
    "stopWords.extend([\"make\",\"mr\",\"de\",\"without\",\"let\",\"rather\",\"upon\",\"within\",\"made\",\"must\",\"much\",\"yet\",\"thought\",\"see\",\"said\",\"us\",\"say\",\"whose\",\"though\",\"every\",\"know\",\"many\",\"will\",\"never\",\"even\",\"found\",\"might\",\"almost\",\"although\",\"indeed\",\"thus\",\"still\",\"this\",\"me\",\"of\",\"may\",\"would\",\"ever\",\"could\",\"shall\",\"come\",\"go\",\"soon\",\"however\",\"become\",\"give\",\"take\",\"well\"])\n",
    "def removeStopWords(stopWords,rvw_txt):\n",
    "    newtxt=' '.join([word for word in rvw_txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "train['text_processed']=[removeStopWords(stopWords,x) for x in train['text_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19579x13759 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 207043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(train.text_processed.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tfidf = LemmaTfidfVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     norm='l1',\n",
    "                                     decode_error='ignore')\n",
    "dtm = tfidf.fit_transform(train['text_processed'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wreath            1\n",
      "daytime           1\n",
      "primeval          1\n",
      "watchin           1\n",
      "hindoo            1\n",
      "implicitly        1\n",
      "inundating        1\n",
      "animalistic       1\n",
      "sterner           1\n",
      "battlefield       1\n",
      "congealed         1\n",
      "lifelessness      1\n",
      "choosing          1\n",
      "textbook          1\n",
      "circulatory       1\n",
      "aimless           1\n",
      "rottin            1\n",
      "dandy             1\n",
      "ploughed          1\n",
      "farms             1\n",
      "sex               1\n",
      "sellin            1\n",
      "fred              1\n",
      "zone              1\n",
      "deathless         1\n",
      "deceived          1\n",
      "caving            1\n",
      "identification    1\n",
      "coral             1\n",
      "kindness          1\n",
      "loser             1\n",
      "virtue            1\n",
      "detestation       1\n",
      "rejuvenated       1\n",
      "bears             1\n",
      "forefather        1\n",
      "nuther            1\n",
      "sided             1\n",
      "mightily          1\n",
      "delighted         1\n",
      "lustrous          1\n",
      "drawback          1\n",
      "construe          1\n",
      "harold            1\n",
      "tardily           1\n",
      "chasing           1\n",
      "islander          1\n",
      "despising         1\n",
      "receipt           1\n",
      "investigated      1\n",
      "suffice           1\n",
      "persist           1\n",
      "turns             1\n",
      "wrigglin          1\n",
      "witless           1\n",
      "observable        1\n",
      "gasoline          1\n",
      "taut              1\n",
      "combine           1\n",
      "intricate         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "words_HPL = train[train.author==\"HPL\"][\"text_processed\"].str.split(expand=True).unstack().value_counts()\n",
    "# data = [go.Bar(x = words_HPL.index.values[2:100],y = words_HPL.values[2:100],marker= dict(colorscale='earth',color = words_HPL.values[2:100]),text=\"Frequency\")]\n",
    "# layout = go.Layout(title='Top 50 words of HPL')\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "# py.iplot(fig, filename='bargraph')\n",
    "print(words_HPL[-60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 50 WORDS FOR TOPIC #0\n",
      "['resolved', 'living', 'chance', 'save', 'poetry', 'attempting', 'far', 'impossible', 'behold', 'speak', 'fell', 'intuition', 'world', 'endured', 'observe', 'old', 'like', 'peculiar', 'absolutely', 'sight', 'similar', 'change', 'somewhat', 'pretty', 'bad', 'suspected', 'hell', 'men', 'little', 'happy', 'annoyed', 'sensation', 'told', 'read', 'tell', 'sort', 'strange', 'state', 'heard', 'unknown', 'seen', 'going', 'speaks', 'great', 'funny', 'earth', 'style', 'stand', 'investigation', 'proved', 'method', 'profound', 'line', 'curious', 'particular', 'easily', 'simple', 'try', 'yes', 'thing']\n",
      "\n",
      "\n",
      "THE TOP 50 WORDS FOR TOPIC #1\n",
      "['believe', 'course', 'strange', 'door', 'great', 'sound', 'voice', 'men', 'object', 'length', 'looked', 'told', 'better', 'room', 'dream', 'hope', 'moment', 'went', 'far', 'father', 'light', 'return', 'perdita', 'mind', 'think', 'change', 'matter', 'heart', 'hand', 'raymond', 'tell', 'world', 'left', 'good', 'hour', 'old', 'mean', 'seen', 'passed', 'place', 'friend', 'little', 'word', 'like', 'year', 'death', 'house', 'long', 'eye', 'life', 'felt', 'fear', 'love', 'heard', 'knew', 'came', 'day', 'time', 'night', 'saw']\n",
      "\n",
      "\n",
      "THE TOP 50 WORDS FOR TOPIC #2\n",
      "['ordinary', 'watched', 'black', 'world', 'follow', 'mean', 'pickman', 'definite', 'beating', 'act', 'free', 'look', 'friday', 'genius', 'sat', 'year', 'new', 'one', 'spoke', 'extract', 'fred', 'age', 'deep', 'little', 'vanquished', 'room', 'business', 'lord', 'poet', 'vulgar', 'nature', 'abhorred', 'divested', 'creation', 'diddles', 'feared', 'sir', 'lived', 'knowing', 'consent', 'lacey', 'life', 'face', 'loved', 'strange', 'succumb', 'missed', 'methodical', 'immortality', 'young', 'animal', 'fool', 'dead', 'respect', 'absent', 'reason', 'god', 'great', 'old', 'man']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(n_components=3,random_state=46)\n",
    "nmf_model.fit(dtm)\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 50 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-60:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one          655\n",
      "little       275\n",
      "time         260\n",
      "man          257\n",
      "first        249\n",
      "great        227\n",
      "two          213\n",
      "long         212\n",
      "length       178\n",
      "whole        176\n",
      "day          176\n",
      "like         173\n",
      "eyes         168\n",
      "head         164\n",
      "far          162\n",
      "nothing      153\n",
      "seemed       150\n",
      "three        150\n",
      "way          149\n",
      "night        146\n",
      "good         141\n",
      "left         140\n",
      "old          139\n",
      "matter       139\n",
      "thing        135\n",
      "point        134\n",
      "hand         133\n",
      "mind         131\n",
      "came         128\n",
      "body         126\n",
      "course       126\n",
      "back         120\n",
      "seen         120\n",
      "fact         119\n",
      "feet         118\n",
      "saw          117\n",
      "room         116\n",
      "words        115\n",
      "idea         115\n",
      "door         115\n",
      "manner       114\n",
      "death        113\n",
      "general      113\n",
      "full         113\n",
      "means        111\n",
      "less         108\n",
      "light        108\n",
      "took         107\n",
      "water        106\n",
      "doubt        106\n",
      "person       106\n",
      "life         105\n",
      "moment       104\n",
      "character    103\n",
      "earth        103\n",
      "air          103\n",
      "nature       103\n",
      "place        101\n",
      "nearly       101\n",
      "house         99\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "words_EAP = train[train.author==\"EAP\"][\"text_processed\"].str.split(expand=True).unstack().value_counts()\n",
    "# data = [go.Bar(x = words_HPL.index.values[2:100],y = words_HPL.values[2:100],marker= dict(colorscale='earth',color = words_HPL.values[2:100]),text=\"Frequency\")]\n",
    "# layout = go.Layout(title='Top 50 words of HPL')\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "# py.iplot(fig, filename='bargraph')\n",
    "print(words_EAP[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
